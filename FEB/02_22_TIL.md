# 02-22(토)

## **1️⃣ LangGraph 개념 이해**

LangGraph는 **LangChain 기반의 상태 머신**으로, 복잡한 워크플로우를 관리하는 데 도움을 줌
### 장점
1. 사이클 지원
2. 세밀한 제어
3. 내장 지속성

단순한 질의응답을 넘어서, **기억(memory)** 및 **다중 경로(branching)** 처리가 가능하게 함

🔹 **핵심 개념**

- `StateGraph`: 워크플로우를 정의하는 그래프 객체
- `Node`: 각 노드에서 특정 함수가 실행됨
- `Edge`: 노드 간의 연결
- `State`: 각 노드에서 관리하는 데이터
- `Memory`: 워크플로우에서 이전 상태를 저장

## **2️⃣ LangGraph 프로젝트 기본 환경 설정**

### **필요한 라이브러리 설치**

```bash
pip install langchain langgraph openai python-dotenv
```

## **3️⃣ OpenAI 모델 설정**

OpenAI의 `gpt-4o` 또는 `gpt-3.5-turbo` 모델을 활용

```python
from dotenv import load_dotenv
import os
from langchain.chat_models import ChatOpenAI

load_dotenv()

llm = ChatOpenAI(
    model_name="gpt-4o",  # gpt-4o 또는 gpt-3.5-turbo 사용 가능
    temperature=0.7  # 창의성 조절
)
```

## **4️⃣ LangGraph에서 상태(State) 정의**

LangGraph에서는 대화 상태를 관리해야 함

예를 들어, **사용자의 메시지와 AI의 응답을 저장하는 State**를 정의

```python
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict, Annotated
from langgraph.graph.message import add_messages
from typing import Sequence

# 상태 정의 (대화 기록 및 언어 설정)
class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str  # 응답 언어 설정
```

## **5️⃣ LangGraph 워크플로우 구축**

### **LangGraph `StateGraph` 생성**

LangGraph의 `StateGraph`를 생성하고, 모델을 호출하는 노드를 추가

```python
from langgraph.graph import StateGraph

# StateGraph 인스턴스 생성
workflow = StateGraph(state_schema=ChatState)
```

## **6️⃣ 모델 호출 함수 구현 (`call_model`)**

LangGraph에서는 각 노드에서 특정 작업을 수행

OpenAI 모델을 호출하는 **`call_model` 함수**를 정의

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage

# 프롬프트 템플릿 설정
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Answer in {language}."),
    MessagesPlaceholder(variable_name="messages"),
])

# 모델을 호출하는 함수
def call_model(state: ChatState):
    # 프롬프트 구성
    prompt_text = prompt_template.format(**state)
    response = llm.invoke(prompt_text)
    
    # 새로운 메시지를 기존 메시지 리스트에 추가
    new_messages = state["messages"] + [AIMessage(content=response)]
    return {"messages": new_messages}
```

## **7️⃣ 노드 추가 및 그래프 연결**

이제 `call_model` 함수를 LangGraph의 노드로 추가하고, 그래프의 시작점을 설정

```python
workflow.add_node("call_model", call_model)
workflow.set_entry_point("call_model")
```

## **8️⃣ 메모리 추가 및 컴파일**

LangGraph는 기본적으로 상태를 저장하지 않기 때문에, `MemorySaver`를 사용해서 상태를 보존

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

## **9️⃣ LangGraph 실행 및 테스트**

LangGraph를 실행하여 모델이 정상적으로 동작하는지 확인

```python
# 입력 메시지 정의
input_messages = [
    HumanMessage(content="Hello, how are you?")
]

# 초기 상태
initial_state = {
    "messages": input_messages,
    "language": "English"
}

# LangGraph 실행
output = app.invoke(initial_state)
print(output["messages"][-1].content)  # 마지막 AI 메시지 출력
```

## **🔟 추가 기능: 메시지 정리 (Trim)**

LangChain에는 **메시지 개수를 제한하는 기능**이 있음

예를 들어, 대화가 너무 길어지면 최근 대화만 남기고 나머지는 제거

```python
from langchain_core.messages import trim_messages

trimmer = trim_messages(
    max_tokens=100,  # 메시지가 100 토큰을 넘으면 삭제
    strategy="last",  # 가장 오래된 메시지부터 삭제
    include_system=True
)
```

`call_model`을 실행하기 전에 `trim_messages`를 사용하여 메시지를 정리

```python
def call_model(state: ChatState):
    # 메시지 정리
    trimmed_messages = trimmer.invoke(state["messages"])

    # 프롬프트 생성
    prompt_text = prompt_template.format(messages=trimmed_messages, language=state["language"])
    response = llm.invoke(prompt_text)
    
    # 새로운 메시지를 기존 메시지 리스트에 추가
    new_messages = trimmed_messages + [AIMessage(content=response)]
    return {"messages": new_messages}
```
