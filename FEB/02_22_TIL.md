# 02-22(í† )

## **1ï¸âƒ£ LangGraph ê°œë… ì´í•´**

LangGraphëŠ” **LangChain ê¸°ë°˜ì˜ ìƒíƒœ ë¨¸ì‹ **ìœ¼ë¡œ, ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ê´€ë¦¬í•˜ëŠ” ë° ë„ì›€ì„ ì¤Œ
### ì¥ì 
1. ì‚¬ì´í´ ì§€ì›
2. ì„¸ë°€í•œ ì œì–´
3. ë‚´ì¥ ì§€ì†ì„±

ë‹¨ìˆœí•œ ì§ˆì˜ì‘ë‹µì„ ë„˜ì–´ì„œ, **ê¸°ì–µ(memory)** ë° **ë‹¤ì¤‘ ê²½ë¡œ(branching)** ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ê²Œ í•¨

ğŸ”¹ **í•µì‹¬ ê°œë…**

- `StateGraph`: ì›Œí¬í”Œë¡œìš°ë¥¼ ì •ì˜í•˜ëŠ” ê·¸ë˜í”„ ê°ì²´
- `Node`: ê° ë…¸ë“œì—ì„œ íŠ¹ì • í•¨ìˆ˜ê°€ ì‹¤í–‰ë¨
- `Edge`: ë…¸ë“œ ê°„ì˜ ì—°ê²°
- `State`: ê° ë…¸ë“œì—ì„œ ê´€ë¦¬í•˜ëŠ” ë°ì´í„°
- `Memory`: ì›Œí¬í”Œë¡œìš°ì—ì„œ ì´ì „ ìƒíƒœë¥¼ ì €ì¥

## **2ï¸âƒ£ LangGraph í”„ë¡œì íŠ¸ ê¸°ë³¸ í™˜ê²½ ì„¤ì •**

### **í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**

```bash
pip install langchain langgraph openai python-dotenv
```

## **3ï¸âƒ£ OpenAI ëª¨ë¸ ì„¤ì •**

OpenAIì˜ `gpt-4o` ë˜ëŠ” `gpt-3.5-turbo` ëª¨ë¸ì„ í™œìš©

```python
from dotenv import load_dotenv
import os
from langchain.chat_models import ChatOpenAI

load_dotenv()

llm = ChatOpenAI(
    model_name="gpt-4o",  # gpt-4o ë˜ëŠ” gpt-3.5-turbo ì‚¬ìš© ê°€ëŠ¥
    temperature=0.7  # ì°½ì˜ì„± ì¡°ì ˆ
)
```

## **4ï¸âƒ£ LangGraphì—ì„œ ìƒíƒœ(State) ì •ì˜**

LangGraphì—ì„œëŠ” ëŒ€í™” ìƒíƒœë¥¼ ê´€ë¦¬í•´ì•¼ í•¨

ì˜ˆë¥¼ ë“¤ì–´, **ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì™€ AIì˜ ì‘ë‹µì„ ì €ì¥í•˜ëŠ” State**ë¥¼ ì •ì˜

```python
from langchain_core.messages import BaseMessage
from typing_extensions import TypedDict, Annotated
from langgraph.graph.message import add_messages
from typing import Sequence

# ìƒíƒœ ì •ì˜ (ëŒ€í™” ê¸°ë¡ ë° ì–¸ì–´ ì„¤ì •)
class ChatState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    language: str  # ì‘ë‹µ ì–¸ì–´ ì„¤ì •
```

## **5ï¸âƒ£ LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶•**

### **LangGraph `StateGraph` ìƒì„±**

LangGraphì˜ `StateGraph`ë¥¼ ìƒì„±í•˜ê³ , ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” ë…¸ë“œë¥¼ ì¶”ê°€

```python
from langgraph.graph import StateGraph

# StateGraph ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
workflow = StateGraph(state_schema=ChatState)
```

## **6ï¸âƒ£ ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜ êµ¬í˜„ (`call_model`)**

LangGraphì—ì„œëŠ” ê° ë…¸ë“œì—ì„œ íŠ¹ì • ì‘ì—…ì„ ìˆ˜í–‰

OpenAI ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” **`call_model` í•¨ìˆ˜**ë¥¼ ì •ì˜

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant. Answer in {language}."),
    MessagesPlaceholder(variable_name="messages"),
])

# ëª¨ë¸ì„ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜
def call_model(state: ChatState):
    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    prompt_text = prompt_template.format(**state)
    response = llm.invoke(prompt_text)
    
    # ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ê¸°ì¡´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    new_messages = state["messages"] + [AIMessage(content=response)]
    return {"messages": new_messages}
```

## **7ï¸âƒ£ ë…¸ë“œ ì¶”ê°€ ë° ê·¸ë˜í”„ ì—°ê²°**

ì´ì œ `call_model` í•¨ìˆ˜ë¥¼ LangGraphì˜ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ê·¸ë˜í”„ì˜ ì‹œì‘ì ì„ ì„¤ì •

```python
workflow.add_node("call_model", call_model)
workflow.set_entry_point("call_model")
```

## **8ï¸âƒ£ ë©”ëª¨ë¦¬ ì¶”ê°€ ë° ì»´íŒŒì¼**

LangGraphëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ìƒíƒœë¥¼ ì €ì¥í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, `MemorySaver`ë¥¼ ì‚¬ìš©í•´ì„œ ìƒíƒœë¥¼ ë³´ì¡´

```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

## **9ï¸âƒ£ LangGraph ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸**

LangGraphë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸

```python
# ì…ë ¥ ë©”ì‹œì§€ ì •ì˜
input_messages = [
    HumanMessage(content="Hello, how are you?")
]

# ì´ˆê¸° ìƒíƒœ
initial_state = {
    "messages": input_messages,
    "language": "English"
}

# LangGraph ì‹¤í–‰
output = app.invoke(initial_state)
print(output["messages"][-1].content)  # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ ì¶œë ¥
```

## **ğŸ”Ÿ ì¶”ê°€ ê¸°ëŠ¥: ë©”ì‹œì§€ ì •ë¦¬ (Trim)**

LangChainì—ëŠ” **ë©”ì‹œì§€ ê°œìˆ˜ë¥¼ ì œí•œí•˜ëŠ” ê¸°ëŠ¥**ì´ ìˆìŒ

ì˜ˆë¥¼ ë“¤ì–´, ëŒ€í™”ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ìµœê·¼ ëŒ€í™”ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°

```python
from langchain_core.messages import trim_messages

trimmer = trim_messages(
    max_tokens=100,  # ë©”ì‹œì§€ê°€ 100 í† í°ì„ ë„˜ìœ¼ë©´ ì‚­ì œ
    strategy="last",  # ê°€ì¥ ì˜¤ë˜ëœ ë©”ì‹œì§€ë¶€í„° ì‚­ì œ
    include_system=True
)
```

`call_model`ì„ ì‹¤í–‰í•˜ê¸° ì „ì— `trim_messages`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì •ë¦¬

```python
def call_model(state: ChatState):
    # ë©”ì‹œì§€ ì •ë¦¬
    trimmed_messages = trimmer.invoke(state["messages"])

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt_text = prompt_template.format(messages=trimmed_messages, language=state["language"])
    response = llm.invoke(prompt_text)
    
    # ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ê¸°ì¡´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    new_messages = trimmed_messages + [AIMessage(content=response)]
    return {"messages": new_messages}
```
