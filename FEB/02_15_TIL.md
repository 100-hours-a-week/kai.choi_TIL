# 02-15(토)

### 👨‍👨‍👧‍👧 군(Group) 개념 복습 With 정수($\mathbb{Z}$)

군이란, 한 집합이 특정 연산에 대해 **폐쇄적(닫혀있다.)이며, 항등원과 역원을 가지며, 결합 법칙이 성립하는 구조**를 의미

집합  $G$와 이항 연산 ∗가 있을 때, $(G,∗)$가 군을 이루려면 다음 네 가지 성질을 만족해야 함

1. 닫힘성
    1. $∀a,b \in \mathbb{Z}, 연산\, +$ 를 수행했을 떄, 결과가 여전히 $\mathbb{Z}$에 속해야 함
    2. $a+b \in \mathbb{Z}$
    3. 상기 식은 자명하게 성립한다. 왜냐하면 정수 간의 덧셈 결과는 정수이기 떄문
2. 결합법칙
    1. $∀a,b,c \in \mathbb{Z}$일 때, 다음이 성립해야 함
    2. $(a+b)+c = a+(b+c)$
    3. 상기 식 또한 정수 간의 덧셈에서는 항상 참임
3. 항등원 
    1. 항등원 $e \in \mathbb{Z}$는 어떤 정수 $a$에 대해서도 다음을 만족하는 원소여야 함
    2. $a+e = e+a = a$
    3. 여기서 $e=0$을 선택하면 위 조건이 항상 성립한다. 즉, 0이 덧셈에 대한 항등원임 
4. 역원
    1. 각 정수 $a \in \mathbb{Z}$에 대해 그에 대한 덧셈 역원 $b \in \mathbb{Z}$가 존재해야 하며, $a+b = b+a =0$를 만족
    2. 이때 $b=-a$로 두면 항상 성립. 즉, 각 정수 a에 대해 $-a$가 존재하므로 역원이 존재

⇒ 위 네가지 조건을 모두 만족하기 때문에 정수 $\mathbb{Z}$가 것셈 연산에 대해 군을 이루는 구조를 가진다고 할 수 있다.(이 군은 가환군이며, 항등원은 0, 역원은 $-a$)

- 뺄셈 = 덧셈의 역원
- 자연수는 덧셈 군인가?? 결합 0 항등 0 역원 추가한 것이 정수
- 유리수: 곱셈 군화
- 나눗셈 = 곱셈의 역원을 정수에 추가

---

### $\mathbb{R}$ (실수 전체 집합)은 벡터 공간일까??

아닐 것 같은 데, 유클리드 평면($\mathbb{R}$ x $\mathbb{R}$ = $\mathbb{R}$^2), $\mathbb{R}$^3: 유클리드 공간

벡터 공간이 되기 위해서는 집합과 연산이 다음 조건을 만족해야 함

1. 덧셈에 대해 닫혀 있음
2. 스칼라 곱셈에 대해 닫혀 있음
    1. 실수 스칼라 $a \in \mathbb{R}$와 벡터 $x \in \mathbb{R}$에 대해 $a\cdot x \in \mathbb{R}$ → 맞음
3. 벡터 공간의 공리(결합법칙, 교환법칙, 분배법칙, 항등원, 역원 등) 만족
    1. 실수 덧셈과 실수 곱셈은 벡터 공간 공리를 모두 만족

⇒ $\mathbb{R}$는 실수체 $\mathbb{R}$ 위에서 벡터 공간을 이룬다는 결론을 얻을 수 있다.

### $\mathbb{R}^n$의 차원과 비교

벡터 공간의 차원의 정의: 기저(집합)의 원소의 개수

- $\mathbb{R}^2$(유클리드 평면):
    - 기저: {(1,0),(0,1)} (두 개의 선형 독립인 벡터)
    - 차원: $dim \, \mathbb{R}^2 = 2$
- $\mathbb{R}^3$(유클리드 공간):
    - 기저: {(1,0,0),(0,1,0),(0,0,1)} (세 개의 선형 독립인 벡터)
    - 차원: $dim \, \mathbb{R}^3 = 3$
- $\mathbb{R}$(수직선):
    - 기저: {1} (1 개의 선형 독립인 벡터)
    - 차원: $dim \, \mathbb{R} = 1$
    - 즉, $\mathbb{R}$는 1차원 벡터 공간임.

유리수: 곱셈 군화

나눗셈: 곱셉의 역뤈을 정수 

미분 = 평균 변화율의 극한

---

### 선형 사상(map,mapping), 변환(Transformation), 함수(function)

선형 사상 또는 선형 변환은 벡터 공간 사이의 함수

입력 벡터를 변환하여 다른 벡터로 보내는 변환 과정으로 특정한 성질을 만족하는 함수.

1. 덧셈 보존
    1. $T(v_1+v_2) = T(v_1) + T(v_2)$
2. 스칼라 곱 보존
    1. $T(cv)=cT(v)$

함수: 집합과 집합의 연결

함수의 구성(이름: 익명), 정의역(실수), 공역(실수), 대응 규칙(필수)

$$
y = x+1
$$

### 준동형 사상과 동형 사상

함수중에 단순히 집합뿐 아니라 그 집합의 구조까지 연결하는 함수가 있을까??

⇒ 준동형 사상

$$
(G, +{\tiny{G}}), (H, +{\tiny{H}})
\\ 
f:G \to H
\\
\vee a,b \in G, \, f(a+{\small{G}}b)= f(a)+f(y)
\\
(\mathbb{Z},+),(\mathbb{Z},+)
\\
f(x) = x
\\
f(x+y) = x+y = f(x) + f(y)
$$

f: 군 준동형 사상

만일 f가 역함수가 있어 그 역함수도 군 준동형 사상이면 그 때 군 동형 사상이라고 함

$$
(G,+{\small{G}}),(H,+{\small{H}}) 
\\
f:G \to H
\\
f^{-1}:H \to G
$$

벡터 공간 준동형 사상

선형성 - 중첩원리

- 가산성
- 동차성

선형변환(함수, 사상): 선형성(벡터합과 스칼라곱)을 보존 - 벡터 공간 준동형 사상

---

### 부분공간

부분 구조: 부분 집합이면서 모집합과 같은 구조를 갖는 것

$$
(\mathbb{Z}, +): 정수 덧셈군 \\
\mathbb{Z}_2 \subset \mathbb{Z}:짝수
\\
(\mathbb{Z}_2, +): 짝수 \, 덧셈군
\\
\mathbb{Z}_2 < \mathbb{Z}
\\
(부분군)
$$

부분 공간: 부분 집합이면서 모(벡터)공간과 같은 구조(즉 같은 연산)인 것

- 반드시 0 벡터 존재
- 그 안에서 베터랍과 스칼라곱에 대해서 닫혀있다.

$$
(V,F,+,\cdot), \quad W \leq V
\Leftrightarrow
\begin{aligned}
&1. \quad \mathbf{0} \in W \quad \text{(영벡터 포함)}\\
&2. \quad \forall x, y \in W, \quad x + y \in W \quad \text{(덧셈에 대해 닫힘)}\\
&3. \quad \forall x \in W, \forall c \in F, \quad cx \in W \quad \text{(스칼라 곱셈에 대해 닫힘)}
\end{aligned}
$$

차원 정리

$$
T:V \to W,선형변환 
\\
N(T) = \{x |x \,\, T = 0,\, x \sin V\}\{영공간(null space, kernel)\} \subset V 
\\
R(T) = \{T(x)| x \in V\}(상공간(image, range)) \subset W 
\\
N(T) < V, R(T) < W 
\\
nullity(T) = dim\,\,(N(T)) 
\\
rank(T) = dim \,\,(R(T))
\\
dim(V) = nullity(T) + rank(T)
$$

### 기저로 표현

선형대수는 기저놀음이다. → 기준

순서 기저와 좌표 벡터

$$
A = \{1,2\} = \{2,1\} 
\beta = \{u_1,u_2,u_3, ..., u_n\}(순서 \, 기저(원래 \, 집합이지만 \, 순서가 \, 필요해서\, 순서가 \, 있다고 \, 가정한다.)) \\
\mathbb{R}^2 \Rightarrow \beta = \{(1,0),(0,1)\} = \{e_1,e_2\} \\
v \in V, \exist! (유일하게 \, 존재한다.){a_n},v = \sum{a_1 u_i}
\\
v=(2,3) \in \mathbb{R}^2 \Rightarrow 2(1,1) + 1(0,1) \Rightarrow \begin{bmatrix}v\end{bmatrix}{\tiny{\beta}} = \begin{bmatrix}2 \\ 1 \end{bmatrix}
\\
[v] {\tiny{\beta}} = \begin{bmatrix}  a_1 \\ a_2 \\ a_3 \\ \cdot \\ \cdot \\ a_n \\ \end{bmatrix}
$$

선형 변환의 (행렬) 표현
모든 선형 변환은 꼭 반드시 행렬로 표현이 가능할까요?? → Yes! →계산이 가능하다 → gpu가
AI 모델(함수): 선형 변환
