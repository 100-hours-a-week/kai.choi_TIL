# 2025-03-04 한 줄 정리

1. 사전학습
- 사전 학습 모델	대량의 파라미터를 학습한 모델을 추가 학습 없이도 활용 준수한 성능을 도출할 수 있는 것이다.
- 사전학습 모델을 통해 모델의 학습 비용을 줄이고, 적은 양의 데이터를 통해서도 태스크에 맞게 우수한 성능을 도출할 수 있다.

2. 전이 학습
- 사전 학습 모델을 새로운 도메인에 적용하기 위해 fine-tunning 혹은 재학습을 한다.
- 학습 데이터 부족 문제와 학습 비용을 절감하기 위해서 사용된다.

3. 미세 조정(Fine Tunning)
- 사전 학습 모델의 일부 또는 전체 레이어를 새로운 데이터 셋에 맞게 추가 학습을 진행하는 것으로 주로 마지막 레이어를 학습에 활용하지만 전체 레이어를 학습하는 것도 가능하다.
- 학습 비용 절감과 사용하고자 하는 태스크(도메인)에 적합한 모델을 개발하기 위해 활용

4. 특징 추출(Feature Extraction)
- 주로 이미지 데이터로 부터 특정 패턴을 추출하기 위해 활용되는 기법으로 합성곱 신경망을 통해서 이미지의 특징을 추출하는 것이다.
- 이미지 Classification 혹은 Segmentation 태스크에서의 정확도를 향상 시키기 위해서는 합성곱 신경망 기반의 특징 추출 기법이 우수한 성능을 보여준다.

5. 과(대)적합
- 학습 데이터에서는 매우 우수한 예측 정확도를 보여주지만 새로운(테스트) 데이터에 대해서는 예측 정확도가 떨어지는 현상이다.
- 인공신경망의 레이어를 깊게 쌓거나(파라미터 수가 많다.) 학습 iteration을 과도하게 많이 수행했을 때 발생한다.

6. 과소적합
- 모델의 학습 과정에서 Loss가 줄어들지 않고 검증 데이터 셋에 대한 정확도가 저하되는 현상으로 데이터의 패턴을 제대로 학습하지 못하는 현상이다.
- 모델의 구조가 충분히 깊지 못하거나 학습률이 크기 때문에 자주 발생하며 학습 횟수가 적은 경우에 발생 가능하다.
