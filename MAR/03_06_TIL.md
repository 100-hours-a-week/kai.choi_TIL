# 03/06(목)

## 딥러닝 모델 구성

### Epoch Process

1. **Data Input (데이터 입력)**
    - 배치(batch) 단위로 데이터를 모델에 공급함
    - 일반적으로 `DataLoader`를 통해 미니배치(mini-batch) 단위로 처리됨
2. **Forward Propagation (순전파)**
    - 입력 데이터가 모델을 통과하면서 예측값(출력값)을 생성함
    - 예: `y_pred = model(x)`
3. **Loss Calculation (손실 계산)**
    - 예측값과 실제 정답(레이블) 간의 오차(손실, loss)를 계산함
    - 예: `loss = loss_function(y_pred, y_true)`
4. **Backward Propagation (역전파)**
    - 손실을 모델의 가중치에 대한 기울기(gradient)로 변환함
    - `loss.backward()`를 호출하여 각 가중치에 대한 미분값(gradient) 계산
5. **Optimizer Step (가중치 업데이트)**
    - 옵티마이저가 기울기를 이용해 가중치를 업데이트함
    - 예: `optimizer.step()`
    - 일반적으로 `optimizer.zero_grad()`를 호출하여 이전 기울기를 초기화함

---

## Transfer Learning

사전 훈련된 모델을 그대로 활용, 추가 튜닝하여 새로운 태스크에 적용함으로써 학습 시간을 단축하고 성능을 향상시키는 기법

### Few-Shot Learning

적은 양의 학습 데이터를 가지고도 모델이 일반화 능력을 획득하도록 하는 기법

데이터 확보가 어려운 경우 소량의 데이터(ex. 5장, 10장) 학습을 진행해 새로운 클래스를 인식하거나, 이전에 접하지 않은 상황에 모델이 잘 대응 하도록 함

### Zero-Shot Learning

**특정 카테고리의 예시가 포함된 데이터를 학습되지 않은 모델을 사용해 이미지 분류를 수행하는 작업**

보지 못했던 데이터에 대해서도 일반화 성능이 우수한 사전학습 모델 활용

---

## L1/L2 정규화

L1,L2 정규화는 가중치에 페널티를 주어 과적합을 방지하는 기법

- $L_1: Loss = MSE + \lambda \sum{|W_i|}$
    - 기존 cost 함수에 가중치의 절대값을 더해주어, 미분 시 weight의 크기에 상관 없이 부호에 따라 일정한 상수값을 더해주거나 뻼
- $L_2:Loss = MSE + \lambda \sum {(W_i)^2}$
    - 가중치 제곱의 합을 더하는 형태로, weight의 형태에 따라 weight의 크기에 따라 weight 값이 큰 값을 더 빠르게 감소시키는 weight decay 기법

## Dropout

딥러닝 모델 학습 중 무작위로 뉴런을 비활성화하여 과적합을 방지하는 방법

신경망이 특정 뉴런에 과도하게 의존하는 것을 방지

각 학습 단계에서 랜덤하게 선택된 뉴런을 일시적으로 제거함으로써, 네트워크가 더 강건하게 데이터의 다양한 패턴을 학습하도록 도움
